{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeUOYktpXtOz"
   },
   "source": [
    "# German-to-English Machine Translation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35m1Uv47jiNX"
   },
   "source": [
    "In this notebook, we will go through an example of building and training a long short-term memory (LSTM) Seq2seq model with and without attention from scratch. It is trained on the [Multi30K](https://arxiv.org/abs/1605.00459) dataset with a training/validation/testing split of 29,000/1,014/1,000 examples. We also implement both greedy and beam search algorithms for inference. For the LSTM + attention model with beam search, the trained model achieves a BLEU score of ~37-38."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acPh_4GwYID0"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7cggO7mjZ5L"
   },
   "source": [
    "Let's install and import the necessary dependencies:\n",
    "* `torch` for modeling and training\n",
    "* `torchtext` for data collection\n",
    "* `sentencepiece` for subword tokenization\n",
    "* `sacrebleu` for BLEU score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4LV8KY6_unfe",
    "outputId": "82967403-2a6d-4f5d-c3a3-be438f2ce797"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade sacrebleu sentencepiece\n",
    "!pip install torchtext==0.6.0\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "import sentencepiece\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import tqdm.notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7GXlxjGBfs_"
   },
   "source": [
    "Now let's confirm we have GPU access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KYpIPtqtwVwh",
    "outputId": "cfde71a9-76b8-4b42-fb53-ff98f95f37b4"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANK-5cMtYSyH"
   },
   "source": [
    "## Loading the Multi30K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7xxBtrFFbLGC",
    "outputId": "4b4824ff-a964-4e38-b56e-3b34a226f7cb"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "Multi30k.urls = [\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\",\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\",\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
    "]\n",
    "\n",
    "extensions = [\".de\", \".en\"]\n",
    "source_field = torchtext.data.Field(tokenize=lambda x: x)\n",
    "target_field = torchtext.data.Field(tokenize=lambda x: x)\n",
    "training_data, validation_data, test_data = torchtext.datasets.Multi30k.splits(\n",
    "    extensions, [source_field, target_field], root=\"/content/\", test=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1aBbR3snOjV"
   },
   "source": [
    "## Creating the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Mo0tnIbnQz8"
   },
   "source": [
    "In this project, we use the `sentencepiece` tokenizer to create a joint German-English subword vocabulary. This is a relatively small dataset so we will use a small vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8xSUaso9vo1V",
    "outputId": "a82a5324-b6f1-4946-c757-37682d6d4134"
   },
   "outputs": [],
   "source": [
    "# Define the special tokens\n",
    "args = {\n",
    "  \"pad_id\": 0,\n",
    "  \"bos_id\": 1,\n",
    "  \"eos_id\": 2,\n",
    "  \"unk_id\": 3,\n",
    "  \"input\": \"/content/multi30k/train.de,/content/multi30k/train.en\",\n",
    "  \"vocab_size\": 8000,\n",
    "  \"model_prefix\": \"multi30k\",\n",
    "}\n",
    "combined_args = \" \".join(\n",
    "    \"--{}={}\".format(key, value) for key, value in args.items())\n",
    "sentencepiece.SentencePieceTrainer.Train(combined_args)\n",
    "\n",
    "# Load the vocabulary from the binary file\n",
    "vocab = sentencepiece.SentencePieceProcessor()\n",
    "vocab.Load(\"multi30k.model\")\n",
    "\n",
    "# Get the vocab size\n",
    "print(\"Vocabulary size:\", vocab.GetPieceSize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsxRb6TOVHn5"
   },
   "source": [
    "Now we define the padding, beginning-of-sentence, and end-of-sentence tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xheKi30BVVJC"
   },
   "outputs": [],
   "source": [
    "pad_id = vocab.PieceToId(\"<pad>\")\n",
    "bos_id = vocab.PieceToId(\"<s>\")\n",
    "eos_id = vocab.PieceToId(\"</s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiSPP3tdyaid"
   },
   "source": [
    "## Baseline sequence-to-sequence model\n",
    "\n",
    "We first write some batched functions to convert our input sentences into the subword indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-bI4Im30Ezp"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def make_batch(sentences):\n",
    "  \"\"\"\n",
    "  Converts a list of sentences into a batch of subword indices.\n",
    "\n",
    "  Args:\n",
    "    sentences: A list of sentences (string types)\n",
    "\n",
    "  Returns:\n",
    "    A LongTensor of size (max_sequence_length, batch_size) containing the\n",
    "    subword indices for the sentences, where max_sequence_length is the length\n",
    "    of the longest sentence as encoded by the subword vocabulary and batch_size\n",
    "    is the number of sentences in the batch. A beginning-of-sentence token\n",
    "    is included before each sequence, and an end-of-sentence token is\n",
    "    included after each sequence. Empty slots at the end of shorter sequences\n",
    "    are filled with padding tokens.\n",
    "  \"\"\"\n",
    "\n",
    "  # Sandwich each sentence with BOS and EOS tokens\n",
    "  sequences = []\n",
    "  for sentence in sentences:\n",
    "    ids = [bos_id] + vocab.EncodeAsIds(sentence) + [eos_id]\n",
    "    sequences.append(torch.tensor(ids, dtype=torch.long))\n",
    "\n",
    "  # Pad the sequences to the max length using pad_sequence\n",
    "  batch = pad_sequence(sequences, padding_value=pad_id)\n",
    "\n",
    "  return batch.to(device)\n",
    "\n",
    "def make_batch_iterator(dataset, batch_size, shuffle=False):\n",
    "  \"\"\"\n",
    "  Make a batch iterator that yields source-target pairs.\n",
    "\n",
    "  Args:\n",
    "    dataset: A torchtext dataset object.\n",
    "    batch_size: An integer batch size.\n",
    "    shuffle: A boolean indicating whether to shuffle the examples.\n",
    "\n",
    "  Returns:\n",
    "    Pairs of tensors constructed by calling the make_batch function on the\n",
    "    source and target sentences in the current group of examples. The max\n",
    "    sequence length can differ between the source and target tensor, but the\n",
    "    batch size will be the same.\n",
    "  \"\"\"\n",
    "  # Convert to list\n",
    "  examples = list(dataset)\n",
    "\n",
    "  # Shuffle if you want\n",
    "  if shuffle:\n",
    "    random.shuffle(examples)\n",
    "\n",
    "  # Loop over batches of language pairs\n",
    "  for start_index in range(0, len(examples), batch_size):\n",
    "    example_batch = examples[start_index:start_index + batch_size]\n",
    "    source_sentences = [example.src for example in example_batch]\n",
    "    target_sentences = [example.trg for example in example_batch]\n",
    "    yield make_batch(source_sentences), make_batch(target_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGIIy0230Zdc"
   },
   "source": [
    "Here we define the baseline Seq2seq model. It consists of a bidirectional LSTM encoder that encodes the input sentence into a fixed-size representation, and an LSTM decoder that uses this representation to produce the output sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhmnkF2DkNkb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Baseline model class\n",
    "class Seq2seqBaseline(nn.Module):\n",
    "  def __init__(self, vocab_size=8000, embedding_dim=512, hidden_size=512,\n",
    "               num_layers=2, dropout=0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    # Embedding layer shared by encoder and decoder\n",
    "    # Converts indices to embeddings\n",
    "    # Dimension: (# batch_size) x (vocab_size) x (embedding_dim)\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
    "\n",
    "    # Bidirectional LSTM encoder\n",
    "    self.encoder = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers,\n",
    "      dropout=dropout if num_layers > 1 else 0, bidirectional=True)\n",
    "\n",
    "    # Decoder LSTM (unidirectional)\n",
    "    self.decoder = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers,\n",
    "      dropout=dropout if num_layers > 1 else 0, bidirectional=False)\n",
    "\n",
    "    # Projection to convert encoder hidden state to decoder initial hidden state\n",
    "    # size (hidden_size*2 per layer b/c encoder is bidirectional)\n",
    "    self.hidden_proj_h = nn.Linear(hidden_size * 2, hidden_size)\n",
    "    self.hidden_proj_c = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    # Output projection to vocab size\n",
    "    self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "  def encode(self, source):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      source: (max_source_length, batch_size)\n",
    "    Returns:\n",
    "      encoder_output: (max_source_length, batch_size, hidden_size * 2)\n",
    "      encoder_mask: (max_source_length, batch_size) bool tensor\n",
    "      encoder_hidden: tuple of (h_n, c_n), (num_layers, batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    # Get the max_source_length\n",
    "    lengths = torch.sum(source != pad_id, dim=0)\n",
    "\n",
    "    # Embed (max_source_length, batch_size, embedding_dim)\n",
    "    embedded = self.embedding(source)\n",
    "\n",
    "    # Pack padded sequence for variable lengths\n",
    "    # Ignores the padding tokens to make it more efficient\n",
    "    packed = pack_padded_sequence(embedded, lengths.cpu(), enforce_sorted=False)\n",
    "\n",
    "    # Encode the packed sequence\n",
    "    packed_output, (h_n, c_n) = self.encoder(packed)\n",
    "\n",
    "    # Unpack outputs (max_len, batch_size, hidden_size*2)\n",
    "    encoder_output, _ = pad_packed_sequence(packed_output)\n",
    "\n",
    "    # Create mask where padding tokens are True\n",
    "    encoder_mask = (source == pad_id)\n",
    "\n",
    "    # h_n shape: (num_layers * 2, batch_size, hidden_size)\n",
    "    # Split forward and backward states (num_layers, batch_size, hidden_size)\n",
    "    h_forward = h_n[0::2]\n",
    "    h_backward = h_n[1::2]\n",
    "    c_forward = c_n[0::2]\n",
    "    c_backward = c_n[1::2]\n",
    "\n",
    "    # Concatenate (along hidden_size dim)\n",
    "    # (num_layers, batch_size, hidden_size*2)\n",
    "    h_cat = torch.cat([h_forward, h_backward], dim=2)\n",
    "    c_cat = torch.cat([c_forward, c_backward], dim=2)\n",
    "\n",
    "    # Project to decoder hidden size\n",
    "    # (num_layers, batch_size, hidden_size)\n",
    "    h_proj = self.hidden_proj_h(h_cat)\n",
    "    c_proj = self.hidden_proj_c(c_cat)\n",
    "\n",
    "    # Get hidden encoder state\n",
    "    encoder_hidden = (h_proj, c_proj)\n",
    "\n",
    "    return encoder_output, encoder_mask, encoder_hidden\n",
    "\n",
    "  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      decoder_input: (max_decoder_len, batch_size)\n",
    "      initial_hidden: tuple (h_0, c_0), each (num_layers, batch_size,\n",
    "      hidden_size)\n",
    "      encoder_output and encoder_mask not used here.\n",
    "\n",
    "    Returns:\n",
    "      logits: (max_decoder_len, batch_size, vocab_size)\n",
    "      decoder_hidden: same shape as initial_hidden\n",
    "      attention_weights: None (for baseline)\n",
    "    \"\"\"\n",
    "    # Embed the decoder input\n",
    "    embedded = self.embedding(decoder_input)\n",
    "\n",
    "    # Decode using the initial hidden state\n",
    "    output, decoder_hidden = self.decoder(embedded, initial_hidden)\n",
    "\n",
    "    # Project output to vocab size\n",
    "    logits = self.output_projection(output)\n",
    "\n",
    "    return logits, decoder_hidden, None\n",
    "\n",
    "  def compute_loss(self, source, target):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss for prediction on target sequences.\n",
    "\n",
    "    Args:\n",
    "      source: (max_source_length, batch_size)\n",
    "      target: (max_target_length, batch_size)\n",
    "    \"\"\"\n",
    "    # Encode source\n",
    "    encoder_output, encoder_mask, encoder_hidden = self.encode(source)\n",
    "\n",
    "    # Prepare decoder input and target output by removing the last and first\n",
    "    # tokens from the target, respectively\n",
    "    # We want the decoder_input to predict target_output as closely as possible\n",
    "    decoder_input = target[:-1]\n",
    "    target_output = target[1:]\n",
    "\n",
    "    # Get the logits\n",
    "    logits, _, _ = self.decode(decoder_input, encoder_hidden, encoder_output,\n",
    "                               encoder_mask)\n",
    "    # Flatten for loss\n",
    "    logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "    target_flat = target_output.reshape(-1)\n",
    "\n",
    "    # Calculate the loss using cross entropy\n",
    "    loss = F.cross_entropy(logits_flat, target_flat, ignore_index=pad_id)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRrSLKxKpaa7"
   },
   "source": [
    "For training the Seq2seq model, we use cross-entropy as our loss function. Specifically, for each time step we compare the target token and the token with the highest predicted probability. For parameter optimization, we use Adam with a learning rate of 1e-3 and a weight decay of 1e-5 (to prevent overfitting). We also evaluate the token-level accuracy to see how well the current set of parameters do on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKc1pwaZzcUN"
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs, batch_size, model_file):\n",
    "  \"\"\"\n",
    "  Train the model and save its best checkpoint.\n",
    "\n",
    "  Model performance across epochs is evaluated using token-level accuracy on the\n",
    "  validation set. The best checkpoint obtained during training will be stored on\n",
    "  disk and loaded back into the model at the end of training.\n",
    "  \"\"\"\n",
    "  # Using Adam as the optimizer\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "  # Initialize the accuracy\n",
    "  best_accuracy = 0.0\n",
    "\n",
    "  # Using tqdm library to visualize progress\n",
    "  for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
    "\n",
    "    with tqdm.notebook.tqdm(\n",
    "      # Make batches of the training data\n",
    "      make_batch_iterator(training_data, batch_size, shuffle=True),\n",
    "      desc=\"epoch {}\".format(epoch + 1), unit=\"batch\",\n",
    "      total=math.ceil(len(training_data) / batch_size)) as batch_iterator:\n",
    "\n",
    "      # Enable dropout in training\n",
    "      model.train()\n",
    "\n",
    "      # Initialize loss\n",
    "      total_loss = 0.0\n",
    "\n",
    "      # Loop over batches\n",
    "      for i, (source, target) in enumerate(batch_iterator, start=1):\n",
    "\n",
    "        # Zero out the gradients for the new batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Do a forward pass and compute the loss\n",
    "        loss = model.compute_loss(source, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        batch_iterator.set_postfix(mean_loss=total_loss / i)\n",
    "\n",
    "      # Predict the next tokens (those with the highest probability)\n",
    "      validation_accuracy = token_level_accuracy(model, validation_data)\n",
    "\n",
    "      # Show mean loss updates\n",
    "      batch_iterator.set_postfix(mean_loss=total_loss / i,\n",
    "        validation_token_accuracy=validation_accuracy)\n",
    "\n",
    "      # Save model if our validation accuracy improves\n",
    "      if validation_accuracy > best_accuracy:\n",
    "        print(\"New best validation accuracy of {:.2f}, saving model \"\n",
    "              \"checkpoint to {}\".format(validation_accuracy, model_file))\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        best_accuracy = validation_accuracy\n",
    "\n",
    "  print(\"Reloading best model checkpoint from {}\".format(model_file))\n",
    "  model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "def token_level_accuracy(model, dataset, batch_size=64):\n",
    "  \"\"\"\n",
    "  Computes token-level accuracy on the validation set.\n",
    "\n",
    "  \"\"\"\n",
    "  # Switch to inference\n",
    "  model.eval()\n",
    "\n",
    "  # Initialize for accuracy metrics\n",
    "  total_predictions = 0\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "\n",
    "    # Loop over batches of language pairs\n",
    "    for source, target in make_batch_iterator(dataset, batch_size):\n",
    "\n",
    "      # Encode the source\n",
    "      encoder_output, encoder_mask, encoder_hidden = model.encode(source)\n",
    "\n",
    "      # Get the target tokens for each time step\n",
    "      decoder_input, decoder_target = target[:-1], target[1:]\n",
    "\n",
    "      # Get the probability distribution of predicted tokens\n",
    "      logits, decoder_hidden, attention_weights = model.decode(\n",
    "        decoder_input, encoder_hidden, encoder_output, encoder_mask)\n",
    "\n",
    "      # Get the total number of predictions by omitting padding tokens\n",
    "      total_predictions += (decoder_target != pad_id).sum().item()\n",
    "\n",
    "      # Get number of correct predictions\n",
    "      correct_predictions += ((decoder_target != pad_id) &\n",
    "        (decoder_target == logits.argmax(2))).sum().item()\n",
    "\n",
    "  # Get the total accuracy on the validation set\n",
    "  accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZYFBChe8-2F"
   },
   "source": [
    "Now we go ahead and train the baseline model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548,
     "referenced_widgets": [
      "20b478fdbadc442884a1d1a030c8f076",
      "0fb669459cac4a22a98c9bf3324d0a34",
      "a17c6ddaf73242f8808a0a5577901b37",
      "0af7e01deb964c259b3eb586731ecf35",
      "d09634b9dd254b239408571a53613278",
      "e5544534064948638247a4f42be5e3c4",
      "223157da34734ae9909befd3ce03310c",
      "a4485e12e90843c4a6ef134512f6c399",
      "2e4ad171a11840549398602d1a109c40",
      "de0c713fd859484fb38afabae0fd1fb6",
      "1b82ef7291244d5096fd862248510ff1",
      "39469ce1c4804d3dbec4a0a5c1ea0a1d",
      "9f0b678eedb644d693d60dac363ba866",
      "2a553860cb2f48db97c2fbdc438f5a17",
      "4fa47d8548244b3ba7ec12e72e88bfe4",
      "ecee00983af94241897b29cbe70d20fd",
      "ef5c56a5cda642f39b53fc5514f0a0f5",
      "a8cbcbe389164c5e8ed2cea28174e179",
      "1a764a9419064d00b8d11319eb45c517",
      "770699420ad84779b63bdc214c562466",
      "3736a85c49eb4462a967353ddf5f0902",
      "1c82eab747314bd3ac4a53727804c0d6",
      "b81ee410e4ef4a15828b2b9b227c688b",
      "d4c4eba50de3410ea2bd6b6a2e73196a",
      "7ffa815af7474accb62695163e572c49",
      "083c7efbce9340b19cf5c6530c0aa399",
      "6bf33285ec804d6f8ace8e6bcbe6a956",
      "b72f8db48d844d099a153ace9b0764c4",
      "c06479676bac4206b0a353b217ab73bc",
      "44bd035c8b784d9394f39102ad8ea2da",
      "512ac9d6b5654b1bbd2d0b8495ca8e8b",
      "95086fc9cc9543ec8ad800707f06c881",
      "727cfe16878d49d49381a28816063ed2",
      "4612205871fe479aa2c24e934cfe5219",
      "d0dbbb1699df428a9eaf094b4d045010",
      "c6bab353e240448ba9078a4e4fee17b9",
      "b9880afdb47c483aa3aca3deec5a931a",
      "f643b304e5484e86b25ae5d2cb093e34",
      "d6f8c71d6daa4e7c98e2793a10e56de6",
      "65ee643dcc094fe7baee9a91e5e4a2f9",
      "d2201c3a2253443387e53fc2948e4c2b",
      "e47dc47fbc324a9799b2a23a7ed1963f",
      "0b2c5cbb9c844091b7300ee2fdc19237",
      "19ad203e46d944acb63235d551016627",
      "02f8032fd0b54deb9a329cb79a5229f4",
      "27fdd129bb3e422ba657c5f82c64568f",
      "ed433a32147f44788baafedad05b08ef",
      "57f58642ee2945a598399eabb0279d88",
      "328a4a74e1054ce9a692d894c4f0de38",
      "809a53424fc24f3cb1ea0ff577d2a103",
      "52100410950f426f9a840bde8ed55db7",
      "023a1b6103d646009534ac3e0af5abeb",
      "a4024c1ddd52463db7fcbe117dd62299",
      "e04fddd7511a41f09f333f0703b606da",
      "a961ff6ea34247368cb1f6be48d875f1",
      "72c9c504e81449faa0edb3d5c46e2f97",
      "804c0377654d4682a22f80cb26f5998c",
      "e828e46762734585800ae9e582be6dbb",
      "3a9a66fa42a143e79b1867ead172e298",
      "49e8e8d9cfbc4bcba466b9b241669797",
      "475cecd3da994ca5aec6f6a90d25e4f5",
      "8b7a0c85590c48a39585670f86fc5c48",
      "94a006d447724d23a024803c9d11402d",
      "1c25bdea28d74d92b0708868037df704",
      "3383988b544045249e5a81c34d4d0289",
      "767096e6326740b1856de2b94753369f",
      "f76cf25dd1214c3a90bef580526d7631",
      "06fcc3f771384dbc8917bac6024e97c5",
      "c306567e5be6416089686080a93ff0fc",
      "9bade551952a4e918ddfb1d3f205f559",
      "8768f1fd2d5e40feb5006d84099249d0",
      "1162ecd0476e4003a5719786176abe7e",
      "8f7c483d76f54bad9d41345c6a9f7668",
      "3ce38214ce574964896a791f88cececc",
      "e68ecc2054e64ea9938f6f4c7dd12cc4",
      "f1fd27a2df414c65bd8ad53c49596a21",
      "c60119cb1115434eaa333f4a39a83690",
      "24ba076a06ad464fbc14d15005ee82ca",
      "1db98459b6f84cf4998b9b253c66617a",
      "5f93bdaae75e40749ccc61df1c20a790",
      "477a7b5515d742399bab2fcf76728e7e",
      "d7e5f28032204de69c2541a658763c4a",
      "e100db3bf8094b61af216be7619f2a74",
      "f54f8025b38e4d479bb18022f9eaf51c",
      "94ebb86a1d7a41c0a803183841f1ec3d",
      "d6a53fc0aec24bae922776577e77377e",
      "8f2f878f5dc8401f91085d64f2b5f167",
      "ab2bd27f74514bccb8cdcd06134bc5b5",
      "97ac1beaa95f419c8b1a8c5f208e0f76",
      "d9be9abbc7d14467b8e54b436a9aee63",
      "0055d71ab8cf4ac89475ac1d466c7b7b",
      "6ff6429263ba444d8bdb09a2ea6ea605",
      "2d8ec38d7b65436781863b66b3ba29c0",
      "1fd2179bd0764c0b959dc520a9235a4f",
      "eda643edd895452789def7c74adf7067",
      "68b564fc41654d649734ea60ff3541de",
      "5b9438b3f46d444ab1f2789030801d39",
      "fabda5e52f1746fa8f8566e561c48255",
      "6d5a89e5db60482384385be9ed83728a",
      "c1ac7dd2a36a4ad88a42162fca0350ab",
      "266acdb03f704670a3d09db5383731b6",
      "5a6aae6826514531895f089f31dddfa6",
      "37899ebe9ae246bc827a300277113a19",
      "ce5526959bd34cea9290d315987fd9fa",
      "a7df7f583cd94fb59570e283e6324fba",
      "ce051f3464d7497ca7f00fbd6df992ad",
      "032a8ef326014b4aba9308485069d7b4",
      "e98dc6e3e78344779247028d4f250133",
      "6983c25208c74e059e84dd88cedc3052",
      "7d28c48a57de4c8aa4abaf925b37a3ae",
      "f26482a8505e49b09a49556341de6f0b",
      "efc8fa24a96941b2990ab3e309bed0a7",
      "f7dab94771f0453da86dd23eed4d6365",
      "2049582b36f241968134fb9e4f760710",
      "152d480e99ea400ea3397ebb1364357a",
      "edb660e2a00e49fe8743172138ed56f9",
      "964f432fe74e4833a46714594036a505",
      "9185ad9f57ee405d8c8ca080b35ac500",
      "c05b87a8cece4b459b21c3a896171a99",
      "dd842558603e4bf7b53b04f989700f23",
      "864a722e39cc45c8a6999187be4e6ca0"
     ]
    },
    "id": "thNLsZ04Z0A-",
    "outputId": "82b953bf-f77c-4063-b856-15082cdf0b2e"
   },
   "outputs": [],
   "source": [
    "# Choose the number of epochs and batch size\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Initialize model\n",
    "baseline_model = Seq2seqBaseline().to(device)\n",
    "\n",
    "# Now pass the model to the training function\n",
    "train(baseline_model, num_epochs, batch_size, \"baseline_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL7YCTE9vqu9"
   },
   "source": [
    "# Inference with Greedy Search\n",
    "\n",
    "Now we implement greedy search, which is more simple compared to beam search. For every input token, it selects the most probable translation token as predicted by the decoder. This is the most efficient method of inference we explore here. However, the known downside is that each step is irreversible and thus may not be able to find the overall most probable prediction for a full sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RuNVr-5N4QSc",
    "outputId": "e5f89f47-d763-4e3f-8022-37ddc870f122"
   },
   "outputs": [],
   "source": [
    "def predict_greedy(model, sentences, max_length=100):\n",
    "  \"\"\"\n",
    "  Make predictions for the given inputs using greedy inference.\n",
    "\n",
    "  Args:\n",
    "    model: A sequence-to-sequence model\n",
    "    sentences: A list of input sentences (string type)\n",
    "    max_length: The maximum length at which to truncate outputs in order to\n",
    "      avoid non-terminating inference\n",
    "\n",
    "  Returns:\n",
    "    A list of predicted translations (string type)\n",
    "  \"\"\"\n",
    "  # Convert sentences to token indices\n",
    "  batch = make_batch(sentences)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # Encode the source\n",
    "    encoder_output, encoder_mask, encoder_hidden = model.encode(batch)\n",
    "    batch_size = batch.size(1)\n",
    "\n",
    "    # Decoder starts with BOS tokens for all items in batch\n",
    "    seq = torch.full((1, batch_size), bos_id, dtype=torch.long, device=device)\n",
    "\n",
    "    # Bool tensor to keep track of whether each sentence is finished\n",
    "    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "    # Loop over the designated maximum length of our output string\n",
    "    for _ in range(max_length):\n",
    "\n",
    "      # Decode full sequence so far in one call\n",
    "      logits, decoder_hidden, attention_weights = model.decode(\n",
    "          seq, encoder_hidden, encoder_output, encoder_mask)\n",
    "\n",
    "      # Take the logits from the previous step\n",
    "      next_logits = logits[-1, :, :]\n",
    "\n",
    "      # Force the pad token to be chosen if the sentence is completed\n",
    "      if finished.any():\n",
    "        next_logits[finished] += 1e9 * torch.eye((next_logits.size(1)),\n",
    "                                                   device=device)[pad_id]\n",
    "      # Choose the most probable token\n",
    "      # Note we don't have to do softmax here since there is only 1\n",
    "      # inference per input sentence\n",
    "      next_tokens = next_logits.argmax(dim=-1)\n",
    "\n",
    "      # Concatenate to the current sequence\n",
    "      seq = torch.cat([seq, next_tokens.reshape(-1,batch_size)], dim=0)\n",
    "\n",
    "      # Mark finished sequences\n",
    "      finished |= (next_tokens == eos_id)\n",
    "\n",
    "      # End early if all examples in the batch are completed\n",
    "      if finished.all():\n",
    "        break\n",
    "\n",
    "    # Convert token IDs to text\n",
    "    decoded_sentences = []\n",
    "    for i in range(batch_size):\n",
    "      ids = seq[:, i].tolist()\n",
    "      decoded = vocab.DecodeIds(ids)\n",
    "      decoded_sentences.append(decoded)\n",
    "\n",
    "    return decoded_sentences\n",
    "\n",
    "def evaluate(model, dataset, batch_size=64, method=\"greedy\"):\n",
    "  \"\"\"\n",
    "  Evaluate the performance of inference using BLEU score.\n",
    "\n",
    "  \"\"\"\n",
    "  assert method in {\"greedy\", \"beam\"}\n",
    "\n",
    "  # Get source and target sentences from the dataset\n",
    "  source_sentences = [example.src for example in dataset]\n",
    "  target_sentences = [example.trg for example in dataset]\n",
    "\n",
    "  # Inference mode\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  with torch.no_grad():\n",
    "\n",
    "    # Loop over batches and predict translations\n",
    "    for start_index in range(0, len(source_sentences), batch_size):\n",
    "\n",
    "      # If using greedy search\n",
    "      if method == \"greedy\":\n",
    "        prediction_batch = predict_greedy(\n",
    "            model, source_sentences[start_index:start_index + batch_size])\n",
    "\n",
    "      # If beam search\n",
    "      else:\n",
    "        prediction_batch = predict_beam(\n",
    "            model, source_sentences[start_index:start_index + batch_size])\n",
    "        # Take the most probable prediction from beam search\n",
    "        prediction_batch = [candidates[0] for candidates in prediction_batch]\n",
    "      predictions.extend(prediction_batch)\n",
    "\n",
    "  # Calculate BLEU score\n",
    "  return sacrebleu.corpus_bleu(predictions, [target_sentences]).score\n",
    "\n",
    "print(\"Baseline model validation BLEU using greedy search:\",\n",
    "      evaluate(baseline_model, validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT-hp35CDlrY"
   },
   "source": [
    "Let's show some example predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BoevlyGGF8Bs",
    "outputId": "31a83ef0-6778-4c9e-93b0-ff924d3d43eb"
   },
   "outputs": [],
   "source": [
    "def show_predictions(model, num_examples=4, include_beam=False):\n",
    "  \"\"\"\n",
    "  Show some example predictions for funsies.\n",
    "\n",
    "  \"\"\"\n",
    "  for example in validation_data[:num_examples]:\n",
    "    print(\"Input:\")\n",
    "    print(\" \", example.src)\n",
    "    print(\"Target:\")\n",
    "    print(\" \", example.trg)\n",
    "    print(\"Greedy prediction:\")\n",
    "    print(\" \", predict_greedy(model, [example.src])[0])\n",
    "    if include_beam:\n",
    "      print(\"Beam predictions:\")\n",
    "      for candidate in predict_beam(model, [example.src])[0]:\n",
    "        print(\" \", candidate)\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Baseline model sample predictions:\")\n",
    "print()\n",
    "show_predictions(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb7NfD4BD7I_"
   },
   "source": [
    "## Sequence-to-sequence model with attention\n",
    "\n",
    "Now we add an attention mechanism to the model, which mitigates the memory bottleneck of the encoder LSTM's internal hidden state by allowing the model to focus on different parts of the input sequence. We increased the number of layers to better process the context vector while we increased the dropout rate to avoid overfitting. Note that we need to overload the decode function to integrate the attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5i1i2qg_9iA"
   },
   "outputs": [],
   "source": [
    "class Seq2seqAttention(Seq2seqBaseline):\n",
    "  def __init__(self, vocab_size=8000, embedding_dim=512, hidden_size=512,\n",
    "               num_layers=3, dropout=0.4):\n",
    "    super().__init__(vocab_size, embedding_dim, hidden_size, num_layers,\n",
    "                     dropout)\n",
    "\n",
    "    # Project encoder outputs from (2*hidden_size) to hidden_size for attention\n",
    "    # compatibility\n",
    "    self.encoder_proj = nn.Linear(2 * hidden_size, hidden_size)\n",
    "\n",
    "    # Bilinear attention weight matrix (hidden_size x hidden_size)\n",
    "    self.attn_weight = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    # Dropout for the attention output\n",
    "    self.attn_dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    # Project for compatibility with the output_projection\n",
    "    self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      decoder_input: (max_decoder_len, batch_size)\n",
    "      initial_hidden: tuple (h_0, c_0), each (num_layers, batch_size,\n",
    "      hidden_size)\n",
    "      encoder_output: (max_source_len, batch_size, hidden_size*2)\n",
    "      encoder_mask: (max_source_len, batch_size)\n",
    "\n",
    "    Returns:\n",
    "      logits: (max_decoder_len, batch_size, vocab_size)\n",
    "      decoder_hidden: same shape as initial_hidden\n",
    "      attention_weights: (max_decoder_len, batch_size, max_source_len)\n",
    "    \"\"\"\n",
    "    # Embed the decoder input\n",
    "    embedded = self.embedding(decoder_input)\n",
    "    max_decoder_len, batch_size, _ = embedded.size()\n",
    "    max_source_len = encoder_output.size(0)\n",
    "\n",
    "    # Run the decoder\n",
    "    decoder_output, decoder_hidden = self.decoder(embedded, initial_hidden)\n",
    "\n",
    "    # Project encoder outputs to hidden_size for bilinear attention\n",
    "    encoder_proj = self.encoder_proj(encoder_output)\n",
    "\n",
    "    # Project decoder outputs using the bilinear weight matrix\n",
    "    decoder_proj = self.attn_weight(decoder_output)\n",
    "\n",
    "    # Compute attention scores using einsum\n",
    "    attn_logits = torch.einsum('tbh,sbh->tsb', decoder_proj, encoder_proj)\n",
    "    attn_logits = attn_logits.permute(0, 2, 1)\n",
    "\n",
    "    # Mask out padding positions in the encoder\n",
    "    encoder_mask = encoder_mask.transpose(0, 1).unsqueeze(0)\n",
    "    attn_logits.masked_fill_(encoder_mask, -1e9)\n",
    "\n",
    "    # Compute normalized attention weights\n",
    "    attention_weights = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "    # Compute context vectors as weighted sum of encoder_proj\n",
    "    encoder_proj_t = encoder_proj.transpose(0, 1)\n",
    "    context = torch.einsum('tbs,bsh->tbh', attention_weights, encoder_proj_t)\n",
    "\n",
    "    # Combine context with decoder output\n",
    "    decoder_output_combined = self.attn_combine(torch.cat([decoder_output,\n",
    "                                                           context], dim=2))\n",
    "    decoder_output_combined = self.attn_dropout(decoder_output_combined)\n",
    "\n",
    "    # Project to vocabulary size\n",
    "    logits = self.output_projection(decoder_output_combined)\n",
    "\n",
    "    return logits, decoder_hidden, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815,
     "referenced_widgets": [
      "79090f02387a4914bebae0028b637ee6",
      "79de6c1c1cc04519b95e3eef9d23fab4",
      "1730c7b467b54c6da23b1269514df2f0",
      "47752ed320634f959df5350ce61009b7",
      "534c2e571fd4452f84deff8330114374",
      "0a74af035d8e42b0a4058e03beddc6e9",
      "301b571c948c4916ad63404b20e59a9c",
      "222fb5e724c2481795dddae11e4e5f77",
      "7312f621243e4420bf9cbe222024f44e",
      "7f4fea69acc841f9af47f93aabced9c1",
      "4950d961e765446696e362bc8ed8520b",
      "33efb74f00c849539a07b3d8ef496336",
      "72c9f6ed6c834418a470a453490fde62",
      "d99cfa8a5814450fa908505ddd26251a",
      "6f0c649dd9d243bc8ac00b3588a6b493",
      "3e1eed7576be4013aef6754f9a5b4274",
      "7b2e8e9a4e31494caed9b59dc951959a",
      "0c60d05e5ec6418d94dc46a9e72b961b",
      "fb4b428f2b6949b9a2c43edfbd484e94",
      "7e95a31205984d8e9db2bd86262e9298",
      "df10c8f3708943be8df9bee3c8331f73",
      "0a1d5ffb085b49838e904c3772dcb16a",
      "5542d449d7f84155a4688519dcd983b9",
      "f8375f054cf1458792b68f2d5f67829f",
      "5109a68b4bb8431da30f5c2d39b5882e",
      "b63fc4c41c98430eb2e481c2df224d4c",
      "3029b7f2f17c48238e43f25c1379f5dc",
      "1157e0986a824914adc4eea4b0787bf5",
      "049d93345dff403d979740ed56d3c032",
      "349c0d9bcb0447a7bb37ea3c70f4ba75",
      "ff52ec2453ef44ca814dc3cf67c19f1d",
      "074d3cf9252d432ca31799a8ac9f71e1",
      "4377a0339da84fc69f9ee9a0ff45a4e5",
      "c1c5b39fb5514a7ba2e67c66f526e490",
      "58bce88a71564fc9b69ede764913f10a",
      "20e0100a640843b2895960cde2142674",
      "3da4d232a09f49c98eeaafd6de0bc0af",
      "bfae896e917f462a85d7b6c73a69d902",
      "d43fc38fc5154a77abce433b0b6b60a8",
      "40a8143c5f4d4ce999af1a1aecd7e842",
      "926ded19ee594498a55db8d026c881e4",
      "2e96527fb68e4caf8d0aea5a10ff9be4",
      "82ac0d87fb2745f0949a82c2a3fdf7e8",
      "26b8ae89267f4734b9bed29562673326",
      "bf3d274e18cd49eabc6c04dd1c82df96",
      "e750ab7b01474038a4a1dc6035e5c444",
      "2fc5e7c5600848f49a7ffa1973afb84f",
      "51a90d7319f74cb0a6058ee216d3fda9",
      "0be9854ead5143599273b8ae2fad858e",
      "cb01ee15af0f407c84e85b6607d64fd1",
      "02a4b626f15648d79a78c3456e88ffc7",
      "a2ee553a52894040a5137293e40607de",
      "6a510516793f404a9a78442dd93f96a9",
      "734dd40e89804b5f83b977ef00d60fb3",
      "60b93f7585224cc5a5a717a65871f92d",
      "cdc04004b58648cfb2867ea2926cd197",
      "fd0a79d1ae64468a9aed4c840c96ecef",
      "d267b0d143684c45806e6128d460d9dd",
      "8abd65558eec48e38bafb4f461fa3d52",
      "a5b2b5f2f69b4107b967aa57ce72d47b",
      "aa4c008570b74b2b8188edd8df237ab4",
      "2a0616acd06042bbb8377c93673e9e9e",
      "3c897253c50f4de5853cbd4ede6054d5",
      "ff428d2f00734663a1c9de3f14f24d2c",
      "8feb1c773c514a949d4fdcf12dd58403",
      "b35150241337408cb30a0b7c8ebb97bf",
      "c749664d9e504480a363a656043a608f",
      "e093a51dea064b7bb87a0036cc17a036",
      "ceb93cc20ca94226ba3ae1980f617884",
      "701a424560a440bd8fb61ce16a926c5e",
      "b2129f7dda3e4e768bd143648ce52179",
      "dbf505e567c649ecbabd42f6f6950a1d",
      "bffb39884c99455cb181e8c225d457d5",
      "6a6196efac214916a9e9ddbb98cb7097",
      "5fe33444e2504ec4ad300c6a7e6b7d33",
      "57bb17e936cf403bbe86e84fc9bf27e3",
      "c9777135ebbf471faab033354532a16a",
      "0e3fe56feaa34d05ab484c6c70a6673f",
      "f3e7c2036ff04e2e996d88eb3150505a",
      "2909d2179f5f49cdac4538662874ff3a",
      "691a178290534a16bedb0800f83edfa7",
      "17facfeb638b43c9b291613724ab648f",
      "7210e250627b4750b62fcfb94c0a42e9",
      "06dd56fe24cf45d6b6348a6a4961b084",
      "95e5e954a5eb4d7fa8ec039e798fb111",
      "39031616b01645d0a7e18c991ba3a2fd",
      "d3ddd8228127473c938f43e9a7dc89fd",
      "02580aefd45a4fa09d8125df1e39fbc0",
      "d74f4ebdb2c840a38c9fc1249d2e0799",
      "219c029bc9b840b3ab3e7f2a04f43dda",
      "4c93213c53554dea97d140fda8f54f6c",
      "dcea5c63d8c4458e80d6cb4d53b7d32c",
      "1ec4cb9fc3e743ca934a99dfbd124f82",
      "624bfa0985224051914df9e1309c615c",
      "6780d4d47cac454cac1ba7f6a9c1458d",
      "439b06a9201f44188dc867b6d252e162",
      "3e131a92b545472db8775dfad49489e3",
      "bd056fe766304c4aac078cf785dd43f4",
      "150049829e194c96b14cb08c79f18212",
      "fe2aa32ed0064e11933d0d9b33882a63",
      "62923f0dd36d42ddb64005d1668d9fb6",
      "21ce3cbd70224a41a7020e3c8f27bf31",
      "bc3b204d83784baca40c83330281d392",
      "8b1090b087444a9eabecf0f8ddc14897",
      "13eb6a720192411687e8c6e25e80460f",
      "fdfc13da5bb947cf9639ba1a710747b2",
      "2bde0e523b2c466099066bfcabfb6d8f",
      "0ba6c763eba7417dadb841e0d30784bf",
      "d66620a85bf54a39a4ed8da5a9b091c9",
      "7ebee667b5e84560aefb0e7b33753076",
      "159f99a491b84557a3e2754b9c3c9c01",
      "9655c9ce6b714976a2bc9451713a6126",
      "6eb99ea4deeb4da8b272b8e62f0d2afa",
      "240176804b304be2a3fe1c11322596d6",
      "7588d2388d7e485db2c2af2ed86f3aab",
      "1e740ab723614244a9bc0db5d4c05f4b",
      "13e9924cae394991a921aebd3b982fd5",
      "1721394613834ea7a894f5d3c7109cab",
      "3678cbeb7b8646f3a4d06c82030015b7",
      "c0708329629346ebb375df61a0a2cc8c",
      "c34248e4c89347be92c59354d53566bb",
      "e392323b1b5a46fa8b7db6b1c86d4411",
      "f49b6807b64c4377850fba4a9d8be40c",
      "b25e5dc8f48a4b0eaa997ea7e75fa31d",
      "0b9436352c214f11b69ef0ecc6c68b99",
      "1e45cba6df3f46419f9da816f065564a",
      "41e45e9025374f0d91fa1716822ad7be",
      "c2c2520488b648aea7329a38a1c336c3",
      "4d7262f041b143948a6a5f3c0327bc26",
      "db75624dad6c41fb936ea0e4ebe47c71",
      "66c710c4d2334b4ba1c6be4d4949ef2d",
      "0a3e9a22ef4641fb95d33114b6bcbf66",
      "bdcb7ea082f04f78851e9f5737953d89",
      "2a16324351db44b494ab40f96dda0d3f",
      "c96ef35885874e78857899058796de18",
      "a1f10fde70964d81b0113ad5af392120",
      "7c490b23b237480194d36108f28c9a32",
      "62e3335205244502bb35b22f9510391c",
      "ef80f68c41c04b12af321e288b62f1e4",
      "a796b48ee6354b4d98024472244b1ffd",
      "22fc2446606246a0b5ed9f26e4c19a98",
      "f979fffa015548fca656662d7afb82cc",
      "4d75db2d3e334a96b706bd18b14f3786",
      "f8521cf8ca8e44f5b74348171445f828",
      "a7580f30116d4fdbafa605060a09a24f",
      "d967095292554551b637d13d1dc7990b",
      "53a5e38acb384e04b61ad868fb62c242",
      "a7f75750b1de48a99305404ea857d00f",
      "bc87ebf5aa1f4ee79a92ddbf0557d7c8",
      "3433ea75595c4a33ba0c4ae614660c0d",
      "a6f3e0699b274e1a96678ca1f5d1f5f1",
      "b3d911cb09df4f75b822062f5e805906",
      "5096f46eaf314c3f816a5d01125bd16e",
      "ad689513726046388091a6f6eeb1b47e",
      "510303ba72aa45bf81234cc3504ab497",
      "690545030cb74febbeb5ef43eaeedf55",
      "759669bdb4164ddfa09f9c19ed667e29",
      "f216e057e1bb4353b1ec4bab65145e8b",
      "c424bad32d434a34a9abf0cd57ee01ff",
      "893a8dad45654d9db53846571aa745c7",
      "e4be9a4434e8467c961094ae85436f35",
      "ed3e11c66bcb43f79bd14d729bf9e1bb",
      "a5fc1183befd4c23b16d4470dd4fabfe",
      "f9547d26ef3e407e83221d6d7be8ff98",
      "6af90a9819744c0099d9d75a826616a9",
      "e9e850d482b2495a840c5ef05e7b9b1d",
      "9be75960fc3e43b99fb9428779513b84",
      "3629a73d9d3d4581ada80ca6d2e35b87",
      "e4cadbfb35f945d9a311e357a810e187",
      "9a131c87dea747c982e384c0ef616885",
      "b72af4c06b79433a98db46260b254789",
      "0a8ef442c4ff431daa82675201366aa7",
      "51bc3c3458b04367b390bf5ec1223e30",
      "663c9678c6dc4c99bdadac171850ba9a",
      "68f0f2f56ca2406f894a5396e4ffd385",
      "e7b1c6f8d9314a34a45f120db18e2c3d"
     ]
    },
    "id": "0YvMnDHxJOZv",
    "outputId": "2cf6158e-f70a-4a4f-9352-f32b26695138"
   },
   "outputs": [],
   "source": [
    "# Set the number of epochs and batch size\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize attention model\n",
    "attention_model = Seq2seqAttention().to(device)\n",
    "\n",
    "# Train\n",
    "train(attention_model, num_epochs, batch_size, \"attention_model.pt\")\n",
    "print(\"Attention model validation BLEU using greedy search:\",\n",
    "      evaluate(attention_model, validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "U2dWb-JrA2bR",
    "outputId": "57848449-c501-4469-b338-b210b0805526"
   },
   "outputs": [],
   "source": [
    "print(\"Attention model validation BLEU using greedy search:\",\n",
    "      evaluate(attention_model, validation_data))\n",
    "print()\n",
    "print(\"Attention model sample predictions:\")\n",
    "print()\n",
    "show_predictions(attention_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u15C5gGLMVSa"
   },
   "source": [
    "## Inference with Beam Search\n",
    "\n",
    "Now we implement the more complex beam search decoder. In contrast to greedy search, which always chooses the most probable token as the next in the sequence, beam search saves the k most probable at each time step. The probabilities of each sentence are defined by the sum of the log probabilities of each individual token. The total scores are then normalized to account for predictions of variable length. Although more computationally intensive, it should perform better than greedy search because it looks at the scores of the entire sequences versus of the individual tokens at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-n3h47bJ8sz-",
    "outputId": "168eccb6-6136-4e99-baa3-faf932a4992e"
   },
   "outputs": [],
   "source": [
    "def predict_beam(model, sentences, k=5, max_length=100):\n",
    "  \"\"\"\n",
    "  Make predictions for the given inputs using beam search.\n",
    "\n",
    "  Args:\n",
    "    model: A sequence-to-sequence model.\n",
    "    sentences: A list of input sentences, represented as strings.\n",
    "    k: The size of the beam.\n",
    "    max_length: The maximum length at which to truncate outputs in order to\n",
    "      avoid non-terminating inference.\n",
    "\n",
    "  Returns:\n",
    "    A list of beam predictions. Each element in the list is a list of k\n",
    "    strings corresponding to the top k predictions for the corresponding input,\n",
    "    sorted in descending order by score.\n",
    "  \"\"\"\n",
    "\n",
    "  batch = make_batch(sentences)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # Encode all inputs at once\n",
    "    encoder_output, encoder_mask, encoder_hidden = model.encode(batch)\n",
    "    batch_size = batch.size(1)\n",
    "\n",
    "    # Decoder starts with BOS tokens for all items in batch\n",
    "    seq = torch.full((1, batch_size), bos_id, dtype=torch.long, device=device)\n",
    "    scores = torch.full((1, batch_size), 0.0, dtype=torch.long, device=device)\n",
    "\n",
    "    # Tensor that keeps track of finished sequences\n",
    "    finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "\n",
    "    # Decode\n",
    "    logits, decoder_hidden, attention_weights = model.decode(\n",
    "    seq, encoder_hidden, encoder_output, encoder_mask)\n",
    "\n",
    "    # Get the logits from the previous time step and apply softmax\n",
    "    next_logits = logits[-1, :, :]\n",
    "    next_logits = F.softmax(next_logits, dim=1)\n",
    "\n",
    "    # Get the top k tokens\n",
    "    topk_values, topk_indices = torch.topk(next_logits, k=k, dim=1)\n",
    "\n",
    "    # Expand to account for multiple inferences per sentence\n",
    "    seq = seq.repeat_interleave(k, dim=1)\n",
    "    scores = scores.repeat_interleave(k, dim=1)\n",
    "    scores = scores + torch.log(topk_values.reshape(-1,batch_size*k))\n",
    "    seq = torch.cat([seq, topk_indices.reshape(-1,batch_size*k)], dim=0)\n",
    "    finished = torch.zeros(batch_size * k, dtype=torch.bool, device=device)\n",
    "    finished |= (topk_indices.reshape(batch_size*k) == eos_id) | (\n",
    "        topk_indices.reshape(batch_size*k) == pad_id)\n",
    "\n",
    "    encoder_output = encoder_output.repeat_interleave(k, dim=1)\n",
    "    encoder_mask = encoder_mask.repeat_interleave(k, dim=1)\n",
    "    encoder_hidden = (encoder_hidden[0].repeat_interleave(k, dim=1),\n",
    "                      encoder_hidden[1].repeat_interleave(k, dim=1))\n",
    "\n",
    "    # Loop over time steps\n",
    "    for _ in range(max_length):\n",
    "\n",
    "      # Decode full sequence so far in one call\n",
    "      logits, decoder_hidden, attention_weights = model.decode(\n",
    "          seq, encoder_hidden, encoder_output, encoder_mask)\n",
    "\n",
    "      # Get the logits from the previous time step and apply softmax\n",
    "      next_logits = logits[-1, :, :]\n",
    "\n",
    "      # Pad sequence if finished\n",
    "      if finished.any():\n",
    "        next_logits[finished] += 1e9 * torch.eye((next_logits.size(1)),\n",
    "                                                   device=device)[pad_id]\n",
    "      # Apply softmax\n",
    "      next_logits = F.softmax(next_logits, dim=1)\n",
    "\n",
    "      # Get top k\n",
    "      topk_values, topk_indices = torch.topk(next_logits, k=k, dim=1)\n",
    "\n",
    "      # Repeat along batch_size dimension to account for keeping top k\n",
    "      seq_new = seq.repeat_interleave(k, dim=1)\n",
    "      scores_new = scores.repeat_interleave(k, dim=1)\n",
    "\n",
    "      # Concatenate sequences\n",
    "      seq_new = torch.cat([seq_new, topk_indices.reshape(-1,batch_size*k*k)],\n",
    "                            dim=0)\n",
    "      scores_new = scores_new + torch.log(topk_values.reshape(\n",
    "          -1,batch_size*k*k))\n",
    "\n",
    "      # Keep finished sequences on the beam\n",
    "      if finished.any():\n",
    "        finished_new = finished.repeat_interleave(k, dim=0)\n",
    "        indices = torch.nonzero(finished, as_tuple=False).squeeze()\n",
    "        target_indices = k * indices\n",
    "        finished_new[target_indices] = False\n",
    "        scores_new[0,finished_new] = -1e9\n",
    "\n",
    "      # Choose the top k for each in batch\n",
    "      scores_chunks = scores_new.view(1, batch_size, k*k)\n",
    "      top_vals, top_indices = torch.topk(scores_chunks, k=k, dim=2)\n",
    "      offsets = torch.arange(0, k*k*batch_size, k*k, device=device).view(\n",
    "            1, batch_size, 1)\n",
    "      global_indices = top_indices + offsets\n",
    "      top_vals = top_vals.view(1,-1)\n",
    "      global_indices = global_indices.view(-1)\n",
    "\n",
    "      # Now save selected indices of seq_new to seq and do the same for the\n",
    "      # scores\n",
    "      seq = seq_new[:, global_indices]\n",
    "      scores = scores_new[:, global_indices]\n",
    "\n",
    "      # The sequence is finished if it chooses an EOS or padding token\n",
    "      finished = torch.isin(seq[-1, :], torch.tensor([eos_id, pad_id],\n",
    "                                          device=seq.device)).reshape(-1)\n",
    "      # Break if all predictions finish early\n",
    "      if finished.all():\n",
    "        break\n",
    "\n",
    "    # Normalize over length\n",
    "    nonzero_counts = (seq != 0).sum(dim=0)\n",
    "    # Dive Into Deep Learning mentions using L^a, a = 0.75 for the\n",
    "    # normalization:\n",
    "    # https://d2l.ai/chapter_recurrent-modern/beam-search.html\n",
    "    normalized_scores = scores / nonzero_counts**0.75\n",
    "\n",
    "    # Sort by probability\n",
    "    normalized_scores_reshaped = normalized_scores.view(-1, k)\n",
    "    sorted_vals, sorted_ind = torch.sort(normalized_scores_reshaped, dim=1,\n",
    "                                         descending=True)\n",
    "    offsets = torch.arange(0, k*batch_size, k, device=device).reshape(-1,1)\n",
    "    offsets= offsets.repeat_interleave(k, dim=1)\n",
    "    sorted_ind = sorted_ind + offsets\n",
    "    sorted_ind = sorted_ind.view(-1)\n",
    "    seq = seq[:,sorted_ind]\n",
    "\n",
    "    # Convert token IDs to text\n",
    "    decoded_sentences = []\n",
    "    for i in range(batch_size):\n",
    "      k_sentences = []\n",
    "      for j in range(k):\n",
    "        ids = seq[:, i*k+j].tolist()\n",
    "        decoded = vocab.DecodeIds(ids)\n",
    "        k_sentences.append(decoded)\n",
    "      decoded_sentences.append(k_sentences)\n",
    "\n",
    "  return decoded_sentences\n",
    "\n",
    "print(\"Baseline model validation BLEU using beam search:\",\n",
    "      evaluate(baseline_model, validation_data, method=\"beam\"))\n",
    "print()\n",
    "print(\"Baseline model sample predictions:\")\n",
    "print()\n",
    "show_predictions(baseline_model, include_beam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XI76LuciROgh",
    "outputId": "f0454d92-3211-477a-9b3a-32654485ddc8"
   },
   "outputs": [],
   "source": [
    "print(\"Attention model validation BLEU using beam search:\",\n",
    "      evaluate(attention_model, validation_data, method=\"beam\"))\n",
    "print()\n",
    "print(\"Attention model sample predictions:\")\n",
    "print()\n",
    "show_predictions(attention_model, include_beam=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEQlwh2qELmY"
   },
   "source": [
    "## Visualizing attention: a proxy for word alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "id": "Ok-26cAK7vry",
    "outputId": "e150e93e-cef0-45c1-b8d2-f7b1358321bb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Take a source/target pair and visualize the attention weights using a heatmap\n",
    "source = [\"Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\"]\n",
    "source = make_batch(source)\n",
    "target = [\"A group of men are loading cotton onto a truck\"]\n",
    "target = make_batch(target)\n",
    "\n",
    "# Get the attention weights for this source/target pair\n",
    "attention_model.eval()\n",
    "with torch.no_grad():\n",
    "    encoder_output, encoder_mask, encoder_hidden = attention_model.encode(\n",
    "        source)\n",
    "    decoder_input = target[:-1]\n",
    "    logits, _, attention_weights = attention_model.decode(decoder_input,\n",
    "                                  encoder_hidden, encoder_output, encoder_mask)\n",
    "\n",
    "# Shape to matrix\n",
    "attention_matrix = attention_weights[:, 0, :].cpu().numpy()\n",
    "\n",
    "# Get actual token from sentencepiece so we can visualize\n",
    "source_pieces = vocab.IdToPiece(source[:, 0].tolist())\n",
    "target_pieces = vocab.IdToPiece(target[1:, 0].tolist())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_matrix, xticklabels=source_pieces,\n",
    "            yticklabels=target_pieces, cmap='viridis')\n",
    "plt.xlabel(\"Source Tokens\",fontsize=20)\n",
    "plt.ylabel(\"Target Tokens\",fontsize=20)\n",
    "plt.title(\"Attention Weights\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyA0-kAO6Noq"
   },
   "source": [
    "Very cool! The model gets a lot of the 1-1 word translations correct like \"truck = Lastwagen\" and \"men = Männern.\" It also roughly captures the situation in which there is not a 1-1 mapping like \"lädt = is loading,\" in which case the model predicts \"lädt = are load.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRbvYI-D7g68"
   },
   "source": [
    "#Acknowledgements\n",
    "This project was adapted from UC Berkeley's CS288: Natural Language Processing course: https://cal-cs288.github.io/sp22/\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30162,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
